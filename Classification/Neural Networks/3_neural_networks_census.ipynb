{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('census.pkl', 'rb') as f:\n",
    "    x_census_train, x_census_test, y_census_train, y_census_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_census_train.shape, y_census_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_census_test.shape, y_census_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37834275\n",
      "Iteration 2, loss = 0.31968271\n",
      "Iteration 3, loss = 0.31004436\n",
      "Iteration 4, loss = 0.30390564\n",
      "Iteration 5, loss = 0.29691808\n",
      "Iteration 6, loss = 0.29295018\n",
      "Iteration 7, loss = 0.28954450\n",
      "Iteration 8, loss = 0.28660024\n",
      "Iteration 9, loss = 0.28364970\n",
      "Iteration 10, loss = 0.28223940\n",
      "Iteration 11, loss = 0.27859361\n",
      "Iteration 12, loss = 0.27568915\n",
      "Iteration 13, loss = 0.27472304\n",
      "Iteration 14, loss = 0.27127913\n",
      "Iteration 15, loss = 0.26862404\n",
      "Iteration 16, loss = 0.26619325\n",
      "Iteration 17, loss = 0.26365456\n",
      "Iteration 18, loss = 0.25977089\n",
      "Iteration 19, loss = 0.25806445\n",
      "Iteration 20, loss = 0.25494437\n",
      "Iteration 21, loss = 0.25281916\n",
      "Iteration 22, loss = 0.25030919\n",
      "Iteration 23, loss = 0.24862522\n",
      "Iteration 24, loss = 0.24625192\n",
      "Iteration 25, loss = 0.24480671\n",
      "Iteration 26, loss = 0.24141101\n",
      "Iteration 27, loss = 0.23879820\n",
      "Iteration 28, loss = 0.23549092\n",
      "Iteration 29, loss = 0.23481239\n",
      "Iteration 30, loss = 0.23145948\n",
      "Iteration 31, loss = 0.23041939\n",
      "Iteration 32, loss = 0.22763664\n",
      "Iteration 33, loss = 0.22527187\n",
      "Iteration 34, loss = 0.22376222\n",
      "Iteration 35, loss = 0.22051653\n",
      "Iteration 36, loss = 0.21912204\n",
      "Iteration 37, loss = 0.21591781\n",
      "Iteration 38, loss = 0.21646407\n",
      "Iteration 39, loss = 0.21520587\n",
      "Iteration 40, loss = 0.21166866\n",
      "Iteration 41, loss = 0.20958983\n",
      "Iteration 42, loss = 0.20797014\n",
      "Iteration 43, loss = 0.20791497\n",
      "Iteration 44, loss = 0.20473815\n",
      "Iteration 45, loss = 0.20443046\n",
      "Iteration 46, loss = 0.20205425\n",
      "Iteration 47, loss = 0.20141165\n",
      "Iteration 48, loss = 0.19986961\n",
      "Iteration 49, loss = 0.19691418\n",
      "Iteration 50, loss = 0.19565069\n",
      "Iteration 51, loss = 0.19444663\n",
      "Iteration 52, loss = 0.19284930\n",
      "Iteration 53, loss = 0.19229340\n",
      "Iteration 54, loss = 0.19052059\n",
      "Iteration 55, loss = 0.18953220\n",
      "Iteration 56, loss = 0.18834714\n",
      "Iteration 57, loss = 0.18764219\n",
      "Iteration 58, loss = 0.18670160\n",
      "Iteration 59, loss = 0.18493695\n",
      "Iteration 60, loss = 0.18340547\n",
      "Iteration 61, loss = 0.18401340\n",
      "Iteration 62, loss = 0.18079303\n",
      "Iteration 63, loss = 0.18041820\n",
      "Iteration 64, loss = 0.17880741\n",
      "Iteration 65, loss = 0.17796796\n",
      "Iteration 66, loss = 0.17707194\n",
      "Iteration 67, loss = 0.17401991\n",
      "Iteration 68, loss = 0.17459029\n",
      "Iteration 69, loss = 0.17418285\n",
      "Iteration 70, loss = 0.17325132\n",
      "Iteration 71, loss = 0.17178254\n",
      "Iteration 72, loss = 0.17075815\n",
      "Iteration 73, loss = 0.16856229\n",
      "Iteration 74, loss = 0.16781003\n",
      "Iteration 75, loss = 0.17072818\n",
      "Iteration 76, loss = 0.16716944\n",
      "Iteration 77, loss = 0.16622038\n",
      "Iteration 78, loss = 0.16680446\n",
      "Iteration 79, loss = 0.16395607\n",
      "Iteration 80, loss = 0.16443782\n",
      "Iteration 81, loss = 0.16428311\n",
      "Iteration 82, loss = 0.16124289\n",
      "Iteration 83, loss = 0.16249043\n",
      "Iteration 84, loss = 0.16227781\n",
      "Iteration 85, loss = 0.15940167\n",
      "Iteration 86, loss = 0.15699543\n",
      "Iteration 87, loss = 0.15963682\n",
      "Iteration 88, loss = 0.15830881\n",
      "Iteration 89, loss = 0.15631200\n",
      "Iteration 90, loss = 0.15894756\n",
      "Iteration 91, loss = 0.15639016\n",
      "Iteration 92, loss = 0.15306791\n",
      "Iteration 93, loss = 0.15394704\n",
      "Iteration 94, loss = 0.15331618\n",
      "Iteration 95, loss = 0.14970569\n",
      "Iteration 96, loss = 0.15121936\n",
      "Iteration 97, loss = 0.15018161\n",
      "Iteration 98, loss = 0.14985759\n",
      "Iteration 99, loss = 0.14862798\n",
      "Iteration 100, loss = 0.15010429\n",
      "Iteration 101, loss = 0.15016774\n",
      "Iteration 102, loss = 0.14832315\n",
      "Iteration 103, loss = 0.14915850\n",
      "Iteration 104, loss = 0.14812917\n",
      "Iteration 105, loss = 0.14733372\n",
      "Iteration 106, loss = 0.14623951\n",
      "Iteration 107, loss = 0.14477064\n",
      "Iteration 108, loss = 0.14366079\n",
      "Iteration 109, loss = 0.14590714\n",
      "Iteration 110, loss = 0.14469223\n",
      "Iteration 111, loss = 0.14491204\n",
      "Iteration 112, loss = 0.14418322\n",
      "Iteration 113, loss = 0.14279449\n",
      "Iteration 114, loss = 0.14156760\n",
      "Iteration 115, loss = 0.14272047\n",
      "Iteration 116, loss = 0.14041058\n",
      "Iteration 117, loss = 0.13858928\n",
      "Iteration 118, loss = 0.13845786\n",
      "Iteration 119, loss = 0.14033282\n",
      "Iteration 120, loss = 0.13844743\n",
      "Iteration 121, loss = 0.13694544\n",
      "Iteration 122, loss = 0.13754575\n",
      "Iteration 123, loss = 0.13719109\n",
      "Iteration 124, loss = 0.13477285\n",
      "Iteration 125, loss = 0.13820142\n",
      "Iteration 126, loss = 0.13453385\n",
      "Iteration 127, loss = 0.13369040\n",
      "Iteration 128, loss = 0.13338389\n",
      "Iteration 129, loss = 0.13388967\n",
      "Iteration 130, loss = 0.13439883\n",
      "Iteration 131, loss = 0.13413934\n",
      "Iteration 132, loss = 0.13374336\n",
      "Iteration 133, loss = 0.13409311\n",
      "Iteration 134, loss = 0.13256822\n",
      "Iteration 135, loss = 0.13323131\n",
      "Iteration 136, loss = 0.13267175\n",
      "Iteration 137, loss = 0.13273215\n",
      "Iteration 138, loss = 0.12992758\n",
      "Iteration 139, loss = 0.12900094\n",
      "Iteration 140, loss = 0.13174305\n",
      "Iteration 141, loss = 0.13110744\n",
      "Iteration 142, loss = 0.13165072\n",
      "Iteration 143, loss = 0.13096747\n",
      "Iteration 144, loss = 0.13001083\n",
      "Iteration 145, loss = 0.12820254\n",
      "Iteration 146, loss = 0.12729710\n",
      "Iteration 147, loss = 0.12728751\n",
      "Iteration 148, loss = 0.12545222\n",
      "Iteration 149, loss = 0.12823772\n",
      "Iteration 150, loss = 0.12606854\n",
      "Iteration 151, loss = 0.12272294\n",
      "Iteration 152, loss = 0.12505537\n",
      "Iteration 153, loss = 0.12300482\n",
      "Iteration 154, loss = 0.12338238\n",
      "Iteration 155, loss = 0.12525887\n",
      "Iteration 156, loss = 0.12678318\n",
      "Iteration 157, loss = 0.12484790\n",
      "Iteration 158, loss = 0.12394139\n",
      "Iteration 159, loss = 0.12319112\n",
      "Iteration 160, loss = 0.12248671\n",
      "Iteration 161, loss = 0.12046181\n",
      "Iteration 162, loss = 0.12182866\n",
      "Iteration 163, loss = 0.12133104\n",
      "Iteration 164, loss = 0.12332065\n",
      "Iteration 165, loss = 0.12378004\n",
      "Iteration 166, loss = 0.12421271\n",
      "Iteration 167, loss = 0.12393176\n",
      "Iteration 168, loss = 0.12660286\n",
      "Iteration 169, loss = 0.11895223\n",
      "Iteration 170, loss = 0.12291697\n",
      "Iteration 171, loss = 0.12110930\n",
      "Iteration 172, loss = 0.11792036\n",
      "Iteration 173, loss = 0.11780555\n",
      "Iteration 174, loss = 0.11697047\n",
      "Iteration 175, loss = 0.11506282\n",
      "Iteration 176, loss = 0.11591906\n",
      "Iteration 177, loss = 0.11823244\n",
      "Iteration 178, loss = 0.11742243\n",
      "Iteration 179, loss = 0.11726293\n",
      "Iteration 180, loss = 0.11663633\n",
      "Iteration 181, loss = 0.11697337\n",
      "Iteration 182, loss = 0.11793702\n",
      "Iteration 183, loss = 0.11667853\n",
      "Iteration 184, loss = 0.11636031\n",
      "Iteration 185, loss = 0.11580890\n",
      "Iteration 186, loss = 0.11370826\n",
      "Iteration 187, loss = 0.11413503\n",
      "Iteration 188, loss = 0.11396486\n",
      "Iteration 189, loss = 0.11448501\n",
      "Iteration 190, loss = 0.11419738\n",
      "Iteration 191, loss = 0.11314023\n",
      "Iteration 192, loss = 0.11279466\n",
      "Iteration 193, loss = 0.11154313\n",
      "Iteration 194, loss = 0.11260032\n",
      "Iteration 195, loss = 0.11126443\n",
      "Iteration 196, loss = 0.11167497\n",
      "Iteration 197, loss = 0.11050799\n",
      "Iteration 198, loss = 0.11394630\n",
      "Iteration 199, loss = 0.11240708\n",
      "Iteration 200, loss = 0.11048785\n",
      "Iteration 201, loss = 0.10983768\n",
      "Iteration 202, loss = 0.11127055\n",
      "Iteration 203, loss = 0.11132019\n",
      "Iteration 204, loss = 0.11009505\n",
      "Iteration 205, loss = 0.11207072\n",
      "Iteration 206, loss = 0.10755126\n",
      "Iteration 207, loss = 0.11101614\n",
      "Iteration 208, loss = 0.11052128\n",
      "Iteration 209, loss = 0.10971764\n",
      "Iteration 210, loss = 0.10820201\n",
      "Iteration 211, loss = 0.10872448\n",
      "Iteration 212, loss = 0.10823893\n",
      "Iteration 213, loss = 0.10831976\n",
      "Iteration 214, loss = 0.10910868\n",
      "Iteration 215, loss = 0.11165692\n",
      "Iteration 216, loss = 0.10682575\n",
      "Iteration 217, loss = 0.10513932\n",
      "Iteration 218, loss = 0.10542258\n",
      "Iteration 219, loss = 0.10485357\n",
      "Iteration 220, loss = 0.10761959\n",
      "Iteration 221, loss = 0.10551642\n",
      "Iteration 222, loss = 0.10538422\n",
      "Iteration 223, loss = 0.10527136\n",
      "Iteration 224, loss = 0.10604568\n",
      "Iteration 225, loss = 0.10657665\n",
      "Iteration 226, loss = 0.10496037\n",
      "Iteration 227, loss = 0.10558123\n",
      "Iteration 228, loss = 0.10880917\n",
      "Iteration 229, loss = 0.10520347\n",
      "Iteration 230, loss = 0.10326416\n",
      "Iteration 231, loss = 0.10555423\n",
      "Iteration 232, loss = 0.10348921\n",
      "Iteration 233, loss = 0.10233201\n",
      "Iteration 234, loss = 0.10301869\n",
      "Iteration 235, loss = 0.10446872\n",
      "Iteration 236, loss = 0.10490796\n",
      "Iteration 237, loss = 0.10315476\n",
      "Iteration 238, loss = 0.10301597\n",
      "Iteration 239, loss = 0.10635236\n",
      "Iteration 240, loss = 0.10348152\n",
      "Iteration 241, loss = 0.10306745\n",
      "Iteration 242, loss = 0.10211658\n",
      "Iteration 243, loss = 0.10392177\n",
      "Iteration 244, loss = 0.10040432\n",
      "Iteration 245, loss = 0.10209283\n",
      "Iteration 246, loss = 0.10244743\n",
      "Iteration 247, loss = 0.10231484\n",
      "Iteration 248, loss = 0.09809658\n",
      "Iteration 249, loss = 0.09915700\n",
      "Iteration 250, loss = 0.10049673\n",
      "Iteration 251, loss = 0.10010918\n",
      "Iteration 252, loss = 0.09929654\n",
      "Iteration 253, loss = 0.09870502\n",
      "Iteration 254, loss = 0.09986909\n",
      "Iteration 255, loss = 0.09903549\n",
      "Iteration 256, loss = 0.09881240\n",
      "Iteration 257, loss = 0.09978188\n",
      "Iteration 258, loss = 0.10055186\n",
      "Iteration 259, loss = 0.09807153\n",
      "Iteration 260, loss = 0.10074426\n",
      "Iteration 261, loss = 0.10308402\n",
      "Iteration 262, loss = 0.09801990\n",
      "Iteration 263, loss = 0.09875161\n",
      "Iteration 264, loss = 0.09660568\n",
      "Iteration 265, loss = 0.09827591\n",
      "Iteration 266, loss = 0.09819674\n",
      "Iteration 267, loss = 0.09580322\n",
      "Iteration 268, loss = 0.10257283\n",
      "Iteration 269, loss = 0.09797224\n",
      "Iteration 270, loss = 0.09693229\n",
      "Iteration 271, loss = 0.09662792\n",
      "Iteration 272, loss = 0.10032069\n",
      "Iteration 273, loss = 0.09838644\n",
      "Iteration 274, loss = 0.09573670\n",
      "Iteration 275, loss = 0.09688674\n",
      "Iteration 276, loss = 0.09373859\n",
      "Iteration 277, loss = 0.09568411\n",
      "Iteration 278, loss = 0.09790499\n",
      "Iteration 279, loss = 0.09510939\n",
      "Iteration 280, loss = 0.09809204\n",
      "Iteration 281, loss = 0.09458845\n",
      "Iteration 282, loss = 0.09696428\n",
      "Iteration 283, loss = 0.09536371\n",
      "Iteration 284, loss = 0.09406530\n",
      "Iteration 285, loss = 0.09675878\n",
      "Iteration 286, loss = 0.09829126\n",
      "Iteration 287, loss = 0.09446483\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_census = MLPClassifier(max_iter= 1000, verbose=True, solver='adam', activation='relu', hidden_layer_sizes=(100,100), tol = 0.00001)\n",
    "neural_network_census.fit(x_census_train, y_census_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = neural_network_census.predict(x_census_test)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_census_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8192425793244626"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy = accuracy_score(y_census_test, predict)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8192425793244626"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEOCAYAAACJlmBtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAASSklEQVR4nO3deZxVdd3A8c8MMwwgW2zikoipPzAXbAwX0BQVSiE0l0wlRTF4NC20XCFaVMjcC5c0xeDhURNzgRDcUMDceIWK4Q9JDTUScZIdZpi5zx/3Bw7DtCF4YO7n/XrxmnPPuffyPXrgM+fec4eiXC6HJEnFWQ8gSdo6GARJEmAQJEmJQZAkAVCS9QCbYtasWWXAl4GFQHXG40jStqIRsAPwUnl5+Zq6G7fJIJCPwfSsh5CkbdShwIy6K7fVICwEmHn2j1m9qCLrWaQNfO/tp/ILH0/IdhCpjspm/Zg3bx6kv0Pr2laDUA2welEFqxYuznoWaQNlZWX5hdKqbAeR6mrceN1SvS+1+6ayJAkwCJKkxCBIkgCDIElKDIIkCTAIkqTEIEiSAIMgSUoMgiQJMAiSpMQgSJIAgyBJSgyCJAkwCJKkxCBIkgCDIElKDIIkCTAIkqTEIEiSAIMgSUoMgiQJMAiSpMQgSJIAgyBJSgyCJAkwCJKkxCBIkgCDIElKDIIkCTAIkqTEIEiSAIMgSUoMgiQJMAiSpMQgSJIAgyBJSgyCJAkwCJKkxCBIkgCDIElKDIIkCTAIkqTEIEiSAIMgSUoMgiQJMAiSpMQgSJIAgyBJSkqyHkBbTlFxMf3uuJK2oTPkckwcMoJcTQ39fv0zKCqi4s13eGTQMHLV1Xxp0EmUDz6FmrVrefbKW3lz0jT63HA5Hbt1AaB5x/as/ngpvzn4mxnvlRqa6uoazvn+3cT5CykqKuK2685g7dpqhlx0DyUljdjzCx2586aBFBcXM/mJV/nJNQ+Ry0H5frsy+hcDKCoqynoXGowtFoQQwlBgEPBhWjUYWACMAzoAy4AzYowfhhDeAbrEGFeHEDoCU4BfxBjHban5CsGe/Y4A4O6e36LTV7rT66qhkMvx5OXXs2D6y/S/eySh3xG8+8fZdL9gAHcccAIlTcoYOGM8bz0+kylDrwaguKSEgTPG8+g5w7PcHTVQjz72JwBmTh7GtBlzueLKCRQXF/GjH/bnmKP347TBtzFp6isc3qMLPxxxH9MeuZR2bVtwzc1/YPFHy2jfrmXGe9BwbFIQQgitgQExxl/+i7uVA9+OMc6q9bgLgddijD8OIZwCDAO+V2v7TsBk4Ecxxoc2ZTZ9Ij78JPMmTgOgdacdWf3xUh4563JyNTUUl5bmv+tfspyduu/LuzP/RHVlFdWVVVTMX8D2+3bhby+/BkD380/nrakzWTRnXoZ7o4bquGPL6dunGwB/ffcjWrdqxhc6d6DiHyvI5XIsW76a0tJGPPfSfPbZa2cuGn4vb/31Qwadfpgx2Mz+qyCEEHoA5wCdgPFp3USgea27/TnGeC75IFyWvuOfFGMcCfQErkn3mwzU/pZzF+Bh4PwY4xObsC+qR666mv5jRtH1+KO5/8QLyNXU0GqXHRnwxN2sWbKcD155g92/dhhrlixb/5jKZSsoa5X/X1pcWkr54FO4s/uJWe2CCkBJSSPOOPcOfj9pFg+M+S4fVSznvIvHcuV1j9CqZTMO79GFCY++zNMz5jJ72k9pvl0TDu17NQd/eXf23L1j1uM3GP9xEEII9wE7A9+JMb6+bn2Mse8/eci9wGhgKfD7EEJfoCWwJG1fBrSqdf8HgJXkX07SZvTwmZfyxCXXMuiF+7llr2NZsuBv/GrPPux/9on0vv5S5k6YSuMW262/f+MW27H643wgdjvqYBY8+xJrli7PanwViHtuOYeff3ASB/b+GStXVTJ90uV8sctOjL7zCS4afi/9vtqNL+/fmY7btwbgsIMDs+csMAib0X9zldFw4HngthDChSGEdpA/QwghTKv165YQQhFwY4xxcYyxEpgE7E8+Di3S87UAPq71/GcBxwGjQghdPtVeCYB9T+9Pz0u/A0DVylXkanJ886HRtNm9E5A/E8jV1PD+i6+yy6HlNCprTFnL5rTv+oX1Lw/tdtQhvDn52cz2QQ3f2PtmMvKGiQA0a1pGcXERbVpvR8sWTQDYsePn+MeSlXxp312ZM/d9Fn+0jLVrq3n+5b+wV9gxy9EbnP/4DCHGOA+4KIRQBpxA/iWj3vWdIYQQWgFzQghdgRVAL+Au8mcAxwAvAl8Dptd62Jz0pvKFwO9CCN1jjKs2cb8EzH1wKv3vHsmZz4yjuLSEKd+/mhUfVtB/zCiqK6uoWrmKRwcNY8UHi3nx5rEMnD6eouIinrriBqrXVALQNnTmld8+lO2OqEH7Rt8DGHj+nRzW92qqqqq58apTadumOacMupWSkkY0Li3hjhvPpEP7lowcfiJ9TroWgJP7d2fvrjtnPH3DUpTL5bbIE4cQBgAXAGuAJ2OMI0IIzYB7gB2ASuDUGOPfa19llB57J9AoxjiwvueeNWvWrsDbT/a7gFULF2+R+aVNNSIX8wsV92Q7iFTHmu1OYc6cOQCdy8vL36m7fYtddhpjHAuMrbNuJXBSPffdtc7tQVtqLklS/fyksiQJMAiSpMQgSJIAgyBJSgyCJAkwCJKkxCBIkgCDIElKDIIkCTAIkqTEIEiSAIMgSUoMgiQJMAiSpMQgSJIAgyBJSgyCJAkwCJKkxCBIkgCDIElKDIIkCTAIkqTEIEiSAIMgSUoMgiQJMAiSpMQgSJIAgyBJSgyCJAkwCJKkxCBIkgCDIElKDIIkCTAIkqTEIEiSAIMgSUoMgiQJMAiSpMQgSJIAgyBJSgyCJAkwCJKkxCBIkgCDIElKDIIkCTAIkqTEIEiSAIMgSUpKsh7g07i7VQUfrP4w6zGkDYxYt9DmjCzHkDa2Zs2/3OwZgrSZtWnTJusRpE2yTZ8hzJ42nLLSqqzHkDbQZvehtGnThor5N2Q9irSBbj1GMW7cuH+63TMESRJgECRJiUGQJAEGQZKUGARJEmAQJEmJQZAkAQZBkpQYBEkSYBAkSYlBkCQBBkGSlBgESRJgECRJiUGQJAEGQZKUGARJEmAQJEmJQZAkAQZBkpQYBEkSYBAkSYlBkCQBBkGSlBgESRJgECRJiUGQJAEGQZKUGARJEmAQJEmJQZAkAQZBkpQYBEkSYBAkSYlBkCQBBkGSlBgESRJgECRJiUGQJAEGQZKUGARJEmAQJEmJQZAkAQZBkpQYBEkSYBAkSYlBkCQBBkGSlBgESRIAJVkPoM/Wog+XUt7rxzw+4QesXFXJkIvuoayslG57f56bRp7G1KfnMOqmSQDkcjDj+XnMmXEVXcOO2Q6uBm3kDRN55LE/UVm1lnPP6sX++3Ta6NgsLi6m/2k3sbhiGaWljWjapDGT778o69EblC0WhBDC8cC1wLtp1QhgOnALsB+wBhgUY5wfQpgGDIkxvhFCaA5MAibHGEdtqfkKUVXVWgZfOIamTUsB+M7QMdw86jQO6b4Hw66awPgHnuf0kw/hq0fuC8AvfvkHenTfwxhoi5o2Yy7PvfQmMydfwcqVlVw7ejK33vV0vcfmm299wOvPXUVRUVHWYzdIn/oloxDCaSGEzvVsKgcujjEenn49AxwHNIkxHgxcClxX57laAo8B9xmDze8HP7qPIQOPYMeOrQF4b2EFh3TfA4AeB+7BjBfmrb/ve+9XMPb+5xhxcf8sRlUBmfLUHPbp+nmOH/BL+p12I317d6v32Pxg0RI+XrKSfqfeSM9jrmLilNnZDt4AbY73EP4OXBdCmBRCODGEUJrWlwNnhRCmhxCuCyGUAD3J/4VPjPF54IBaz9MaeBy4I8Z4y2aYS7WMGT+d9m1b0KfXPuvX7dapA8/MfAOARx+bzYoVa9Zvu/7WKQwd0oeystKNnkvanBZXLOPl2W/zu7vP47Zrz+C0wbfTeZf2Gx2blVVruei8Pjw09gIevOd8hg77PxZ9uDTj6RuWTx2EGOOTMcZvAGcBvYH30qbHgfOBw4DmwBCgJbCk1sOrUygAxgFVwE6fdiZt7K7x03n8mdc5/Osjmf3aAr597h2MHH4iI2+cyJHH/ZwO7VvQrm0LAGpqapg4dTanfOPAjKdWIWj7ueb06bU3jRuXEPbYgSZNSrnxqlM3OjY7dmjFkDN7UVLSiA7tW7L/PrsQ5y/MevwGZXO8ZFQWQvgWMAbYDjg5bborxvhWjDEHPAzsDywFWtT+/WOMa9PypcAxwJkhhK982rm0oWcnXs4zj17GtEcuo9s+u/DbW85h1ivv8L+3D+HJhy7ho4rlHH34FwGYM/d9uuyxA02bNs54ahWCngftyWNPziGXy/G3hf9gxco1zHhh3kbH5hPP/JmTzhoNwPLlq5kz93267un7W5vT5nhT+Vrgr8CAGONigBBCEfBqCOGQGON7wJHALOADoB9wfwjhIOC1Ws8zJ8a4NITw7bT9gBjjos0wn/6JPXbbniOP/znNmpZxRM8uHHP0fgDE+QvZrVP7jKdToejbpxvP/jHS/aifUpOrYfQ1A6isXFvvsTnlqdc4qPdPKS4u5uphJ6w/q9XmUZTL5bbIE4cQegNXAquAPwMXANXkrzLaFygCBqYri6aRrjJKjx0GHA70jjHW1H3uWbNm7Qq8vffOkbLSqi0yv7Sp2uw+FICK+TdkPIm0oa49RjFu3DiAzuXl5e/U3b7FLjuNMU4FptazaUg99z28zu0rycdEkvQZ8ZPKkiTAIEiSEoMgSQIMgiQpMQiSJMAgSJISgyBJAgyCJCkxCJIkwCBIkhKDIEkCDIIkKTEIkiTAIEiSEoMgSQIMgiQpMQiSJMAgSJISgyBJAgyCJCkxCJIkwCBIkhKDIEkCDIIkKTEIkiTAIEiSEoMgSQIMgiQpMQiSJMAgSJISgyBJAgyCJCkxCJIkwCBIkhKDIEkCDIIkKTEIkiTAIEiSEoMgSQIMgiQpMQiSJMAgSJISgyBJAgyCJCkxCJIkwCBIkhKDIEkCDIIkKSnJeoBN1Aigslk/aNw461mkDWy//SgAuvYYlfEk0obatWu3brFRfdu31SDsADBv3rys55A2Mm7cuKxHkP6dHYC/1F25rQbhJeBQYCFQnfEskrStaEQ+Bi/Vt7Eol8t9tuNIkrZKvqksSQIMgiQpMQiSJMAgSJISgyBJAgyCJCkxCAUmhHBqPevKQgi/yWIeqbYQwsX1rGsXQpiaxTyFxiAUnotDCP3W3Qgh7En+Qyp+wE9bg2NCCIPX3QghHArMAqZlNlEB2VY/qaxN91VgSghhOdARGAVcGGOckO1YEgB9gckhhJXATsBA4OQY4wvZjlUY/KRyAQoh7AQ8DqwAjo8xvpfxSNJ6IYQW5I/PSuDYGOOyjEcqGAahwIQQ1v142E7A78h/B/Y6QIyxMqu5JFj/EibA54BxwMV8cnz60yy3MF8yKjwRyAFF6faD6WsO2C2TiaRP3F5r+T3ggrScA3p99uMUFs8QJG2VQgjFMcaarOcoJAahwIQQ2gLDgKOAVsDHwHTgJzHGRRmOJhFC2A24Hignf+VbMfAa8P0Y45tZzlYIfMmo8NwDjAVGAMuAFsAxwHjykZCydCdwWe2rikIIBwFjgB5ZDVUoDELhaRljvK/W7aXAvSGE87IaSKqlSd1LTGOMz4cQspqnoBiEwrMohPAj4DFgCfkzhGPJ/+tzUtZeCSHcxcbH56uZTlUgDELhOR34H+AS8n/YlgIzgTOyHEpKzgWOA3ryyfH5CPBQdiMVDn90ReH5fIzxhhjjCcBNwGxgboxxVbZjSQD0jDH+nvznD14BWgPb499Vnwn/Ixee2wFCCJcAQ4CPgLNDCCMynUrK+0n6OgrYG5gA7A7cnNlEBcSXjApXX6BXjLEqhHAb8Ayf/GGUsnZgjPGwtDw5hPB0ptMUCM8QCk+HEML+5N9EbpHWNQWaZDeStN4uIYTjgSUhhF0BQgg7As0ynapAGITCcydwIfnT8fNCCC2BN8i/nyBl7QfAl4BGwHEhhFbAH4ErMp2qQPhJZRFCaBljXJr1HJKy5RlCgQoh/GrdV2OgrU0IoSSEsDCE0DPrWQqJQShce6WvX8x0Cql+Xyf/+YPB/+6O2nwMgqSt0UBgONA2hNAm62EKhUGQtFVJVxdVpp+++1vgzEwHKiAGoXCtSF+XZzqFtLFDgNFp+UFg1+xGKSxeZSRJAjxDKEghhO+GEM5Oy21DCI9lPZOk7BmEwjQWODUtDyD/j+ZIKnAGoQDFGJcAC0IIXYHjgQcyHknSVsAgFK47gF8DL8YYq7IeRlL2DEKBijE+B7xP+nHYkuRVRpIkwDMESVJiECRJgEGQJCUGQZIEGARJUmIQJEmAQZAkJf8Pu0rVQGvo9jwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(neural_network_census)\n",
    "cm.fit(x_census_train, y_census_train)\n",
    "cm.score(x_census_test, y_census_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.87      0.90      0.88      3693\n",
      "        >50K       0.64      0.58      0.61      1192\n",
      "\n",
      "    accuracy                           0.82      4885\n",
      "   macro avg       0.76      0.74      0.75      4885\n",
      "weighted avg       0.81      0.82      0.82      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_test,predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ae31e9fee54a3dc471954a11e553b300df5afc687cca06ee50e4665b9971c71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
